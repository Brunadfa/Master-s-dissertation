{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e814c1de-a062-4bc2-8479-a6689e485484",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d31725ac-ecbd-4ab9-8a9a-837edb489088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ca46013-b24d-4c97-8e3a-61faf60ef35c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available\n"
     ]
    }
   ],
   "source": [
    "# Import OS libraries\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "# Data handling tools\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "\n",
    "\n",
    "# Deep learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense,  BatchNormalization, Activation, Dropout  \n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam , Adamax\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# from googleapiclient.discovery import build\n",
    "# from google.oauth2.credentials import Credentials\n",
    "# from googleapiclient.http import MediaFileUpload\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"GPU is available\" if tf.config.list_physical_devices('GPU') else \"GPU is not available\")\n",
    "\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18a8beb0-4b76-4677-8b4e-644f372c3163",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Diretório onde estão armazenadas as imagens sem ser separadas por pastas\n",
    "image_dir = r'C:\\Users\\bruna\\OneDrive - Universidade do Minho\\Tese Mestrado em Bioinformática\\AGAR_dataset\\AGAR_dataset\\dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "329fc1ab-c494-4956-affd-8b160fda782c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_image(image_path, target_size=(112, 112)):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize(target_size)  \n",
    "        return np.array(image) / 255.0  \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar a imagem {image_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e1c3531-5379-4150-b51e-84039e408b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar IDs do grupo de treinamento de um arquivo de texto\n",
    "train_ids_file = r'C:\\Users\\bruna\\OneDrive - Universidade do Minho\\Tese Mestrado em Bioinformática\\AGAR_dataset\\AGAR_dataset\\training_lists\\higher_resolution_train.txt'\n",
    "with open(train_ids_file, 'r') as file:\n",
    "    train_ids = [str(id) for id in json.loads(file.read())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8151f636-b3c1-4e08-8edb-e16a6ac7d5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar IDs do grupo de validação de um arquivo de texto\n",
    "val_ids_file = r'C:\\Users\\bruna\\OneDrive - Universidade do Minho\\Tese Mestrado em Bioinformática\\AGAR_dataset\\AGAR_dataset\\training_lists\\higher_resolution_val.txt'\n",
    "with open(val_ids_file, 'r') as file:\n",
    "    val_ids = [str(id) for id in json.loads(file.read())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32b2f655-316b-4a71-8fe0-2f8596d5b02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs de treinamento carregados: ['3382', '6923', '593', '1200', '4373'] ... (5242 no total)\n",
      "IDs de validação carregados: ['4020', '4021', '4029', '5983', '5984'] ... (1748 no total)\n"
     ]
    }
   ],
   "source": [
    "# Verificar se os IDs foram carregados corretamente\n",
    "print(f\"IDs de treinamento carregados: {train_ids[:5]} ... ({len(train_ids)} no total)\")\n",
    "print(f\"IDs de validação carregados: {val_ids[:5]} ... ({len(val_ids)} no total)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56f4f561-355f-44c1-b463-ecc098196b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Listas para armazenar caminhos de imagem e rótulos correspondentes\n",
    "train_data = []\n",
    "val_data = []\n",
    "\n",
    "# Iterar sobre todos os arquivos no diretório\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.endswith('.json'):\n",
    "        json_path = os.path.join(image_dir, filename)\n",
    "        with open(json_path, 'r') as file:\n",
    "            try:\n",
    "                json_data = json.load(file)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Erro ao carregar JSON {json_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        sample_id = str(json_data.get('sample_id', None))\n",
    "        if sample_id is None:\n",
    "            print(f\"ID da amostra ausente no JSON: {json_path}\")\n",
    "            continue\n",
    "\n",
    "        image_filename = f\"{sample_id}.jpg\"\n",
    "        image_path = os.path.join(image_dir, image_filename)\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Arquivo de imagem não encontrado: {image_path}\")\n",
    "            continue\n",
    "        \n",
    "        classes = json_data.get('classes', [])\n",
    "        if not classes:\n",
    "            print(f\"Chave 'classes' ausente ou vazia no JSON: {json_path}\")\n",
    "            continue\n",
    "        \n",
    "        data_tuple = (image_path, classes[0])\n",
    "        if sample_id in train_ids:\n",
    "            train_data.append(data_tuple)\n",
    "        elif sample_id in val_ids:\n",
    "            val_data.append(data_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "696ace66-9155-4fd2-85fe-429df96e44f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de imagens de treinamento carregadas: 5240\n",
      "Número de imagens de validação carregadas: 1746\n"
     ]
    }
   ],
   "source": [
    "# Verificar o número de imagens carregadas\n",
    "print(f\"Número de imagens de treinamento carregadas: {len(train_data)}\")\n",
    "print(f\"Número de imagens de validação carregadas: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8009fb6-9b86-4a78-9e44-5195bc093f65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_images = [data[0] for data in train_data]\n",
    "train_labels = [data[1] for data in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5376928-7eaa-448a-9ca4-40f48418ac1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Separar as imagens e os rótulos (labels) para validação\n",
    "val_images = [data[0] for data in val_data]\n",
    "val_labels = [data[1] for data in val_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4bd4a10a-37d5-40fa-ab84-438bb7f65e87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verificar se as listas estão vazias\n",
    "if len(train_images) == 0:\n",
    "    print(\"Nenhuma imagem de treinamento foi carregada.\")\n",
    "if len(val_images) == 0:\n",
    "    print(\"Nenhuma imagem de validação foi carregada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c082868-4a2d-4c94-a34f-2505a16f4d7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filtrar caminhos de imagem válidos\n",
    "train_images = [img for img in train_images if img is not None]\n",
    "val_images = [img for img in val_images if img is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d063e9a1-0294-435c-afbb-81ab8852cb0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converter as imagens em tensores e normalizar, se necessário\n",
    "train_images = [load_image(image_path, target_size=(112, 112)) for image_path in train_images]\n",
    "val_images = [load_image(image_path, target_size=(112, 112)) for image_path in val_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f04c1e1-4323-47b5-8e32-eeacf3d5a7da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verificar se alguma imagem falhou ao carregar\n",
    "train_images = [img for img in train_images if img is not None]\n",
    "val_images = [img for img in val_images if img is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b6f80c9-2fee-4638-b3b7-93e6b51b1e97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de train_images antes da codificação: (5240, 112, 112, 3)\n",
      "Shape de train_labels antes da codificação: (5240,)\n",
      "Shape de val_images antes da codificação: (1746, 112, 112, 3)\n",
      "Shape de val_labels antes da codificação: (1746,)\n"
     ]
    }
   ],
   "source": [
    "# Verificar o shape das imagens e dos rótulos antes da codificação\n",
    "print(\"Shape de train_images antes da codificação:\", np.array(train_images).shape)\n",
    "print(\"Shape de train_labels antes da codificação:\", np.array(train_labels).shape)\n",
    "print(\"Shape de val_images antes da codificação:\", np.array(val_images).shape)\n",
    "print(\"Shape de val_labels antes da codificação:\", np.array(val_labels).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "44012e76-843a-49e0-9adc-737f75cdf313",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todas as imagens de treinamento têm a forma correta: True\n",
      "Todas as imagens de validação têm a forma correta: True\n"
     ]
    }
   ],
   "source": [
    "def check_image_shapes(image_list):\n",
    "    for i, img in enumerate(image_list):\n",
    "        if not isinstance(img, np.ndarray):\n",
    "            print(f\"Item at index {i} is not a numpy array.\")\n",
    "            return False\n",
    "        if img.shape != (112, 112, 3):\n",
    "            print(f\"Image at index {i} has shape {img.shape} instead of (112, 112, 3).\")\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Verificar shapes das imagens de treinamento\n",
    "train_images_valid = check_image_shapes(train_images)\n",
    "print(f\"Todas as imagens de treinamento têm a forma correta: {train_images_valid}\")\n",
    "\n",
    "# Verificar shapes das imagens de validação\n",
    "val_images_valid = check_image_shapes(val_images)\n",
    "print(f\"Todas as imagens de validação têm a forma correta: {val_images_valid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a1af151b-026a-4eb9-9441-462981d91d0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converter os rótulos em formato adequado (numérico), se necessário\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_labels)\n",
    "val_labels = label_encoder.transform(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5114085-7828-43a8-a443-b17337803eae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de classes: 5\n"
     ]
    }
   ],
   "source": [
    "# Verificar o número de classes\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Número de classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "44c03b99-108f-4218-acda-477729c50601",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de train_labels_one_hot após o one-hot encoding: (5240, 5)\n",
      "Shape de val_labels_one_hot após o one-hot encoding: (1746, 5)\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding dos rótulos após a divisão\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "train_labels_one_hot = one_hot_encoder.fit_transform(train_labels.reshape(-1, 1))\n",
    "val_labels_one_hot = one_hot_encoder.transform(val_labels.reshape(-1, 1))\n",
    "\n",
    "# Verificar os shapes dos rótulos one-hot encoded\n",
    "print(\"Shape de train_labels_one_hot após o one-hot encoding:\", train_labels_one_hot.shape)\n",
    "print(\"Shape de val_labels_one_hot após o one-hot encoding:\", val_labels_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b3cef437-0b65-4918-8154-92a817cc0a9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  Criar o modelo da CNN\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(112, 112, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8355a958-fd8b-443f-bc76-38d0215de16a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  Compilar o modelo\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "16db486c-4b55-4d71-ac6d-be20b17264f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de train_images: (5240, 112, 112, 3)\n",
      "Shape de train_labels_one_hot: (5240, 5)\n",
      "Shape de val_images: (1746, 112, 112, 3)\n",
      "Shape de val_labels_one_hot: (1746, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Verificar as dimensões dos dados\n",
    "print(\"Shape de train_images:\", np.array(train_images).shape)\n",
    "print(\"Shape de train_labels_one_hot:\", train_labels_one_hot.shape)\n",
    "print(\"Shape de val_images:\", np.array(val_images).shape)\n",
    "print(\"Shape de val_labels_one_hot:\", val_labels_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "386fd635-8ae1-47e1-82fd-46ad1f81a3f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "164/164 [==============================] - 70s 415ms/step - loss: 1.4343 - accuracy: 0.3466 - val_loss: 1.3379 - val_accuracy: 0.4032\n",
      "Epoch 2/20\n",
      "164/164 [==============================] - 63s 386ms/step - loss: 1.2229 - accuracy: 0.4866 - val_loss: 1.0729 - val_accuracy: 0.5750\n",
      "Epoch 3/20\n",
      "164/164 [==============================] - 63s 382ms/step - loss: 0.9873 - accuracy: 0.6168 - val_loss: 1.0493 - val_accuracy: 0.5796\n",
      "Epoch 4/20\n",
      "164/164 [==============================] - 64s 389ms/step - loss: 0.9016 - accuracy: 0.6460 - val_loss: 0.8094 - val_accuracy: 0.7062\n",
      "Epoch 5/20\n",
      "164/164 [==============================] - 65s 394ms/step - loss: 0.7882 - accuracy: 0.6956 - val_loss: 0.7750 - val_accuracy: 0.7234\n",
      "Epoch 6/20\n",
      "164/164 [==============================] - 66s 400ms/step - loss: 0.7227 - accuracy: 0.7172 - val_loss: 0.7329 - val_accuracy: 0.7394\n",
      "Epoch 7/20\n",
      "164/164 [==============================] - 64s 391ms/step - loss: 0.6723 - accuracy: 0.7473 - val_loss: 0.6987 - val_accuracy: 0.7468\n",
      "Epoch 8/20\n",
      "164/164 [==============================] - 63s 381ms/step - loss: 0.6121 - accuracy: 0.7750 - val_loss: 0.7079 - val_accuracy: 0.7537\n",
      "Epoch 9/20\n",
      "164/164 [==============================] - 61s 374ms/step - loss: 0.5616 - accuracy: 0.7895 - val_loss: 0.6823 - val_accuracy: 0.7566\n",
      "Epoch 10/20\n",
      "164/164 [==============================] - 62s 381ms/step - loss: 0.5219 - accuracy: 0.8042 - val_loss: 0.7315 - val_accuracy: 0.7589\n",
      "Epoch 11/20\n",
      "164/164 [==============================] - 62s 380ms/step - loss: 0.4687 - accuracy: 0.8195 - val_loss: 0.6915 - val_accuracy: 0.7715\n",
      "Epoch 12/20\n",
      "164/164 [==============================] - 61s 371ms/step - loss: 0.4165 - accuracy: 0.8513 - val_loss: 0.7185 - val_accuracy: 0.7812\n",
      "Epoch 13/20\n",
      "164/164 [==============================] - 61s 369ms/step - loss: 0.3943 - accuracy: 0.8523 - val_loss: 0.7828 - val_accuracy: 0.7847\n",
      "Epoch 14/20\n",
      "164/164 [==============================] - 65s 398ms/step - loss: 0.4026 - accuracy: 0.8569 - val_loss: 0.7274 - val_accuracy: 0.7818\n",
      "Epoch 15/20\n",
      "164/164 [==============================] - 67s 407ms/step - loss: 0.3432 - accuracy: 0.8733 - val_loss: 0.8335 - val_accuracy: 0.7829\n",
      "Epoch 16/20\n",
      "164/164 [==============================] - 65s 398ms/step - loss: 0.2958 - accuracy: 0.8935 - val_loss: 0.8607 - val_accuracy: 0.7869\n",
      "Epoch 17/20\n",
      "164/164 [==============================] - 66s 401ms/step - loss: 0.2825 - accuracy: 0.8969 - val_loss: 0.8416 - val_accuracy: 0.7835\n",
      "Epoch 18/20\n",
      "164/164 [==============================] - 64s 393ms/step - loss: 0.2519 - accuracy: 0.9080 - val_loss: 0.8899 - val_accuracy: 0.7898\n",
      "Epoch 19/20\n",
      "164/164 [==============================] - 67s 408ms/step - loss: 0.2577 - accuracy: 0.9095 - val_loss: 1.0322 - val_accuracy: 0.7784\n",
      "Epoch 20/20\n",
      "164/164 [==============================] - 66s 403ms/step - loss: 0.2326 - accuracy: 0.9187 - val_loss: 0.9450 - val_accuracy: 0.7715\n"
     ]
    }
   ],
   "source": [
    "# Treinar o modelo novamente\n",
    "history = model.fit(\n",
    "    np.array(train_images), \n",
    "    train_labels_one_hot, \n",
    "    epochs=20, \n",
    "    batch_size=32, \n",
    "    validation_data=(np.array(val_images), val_labels_one_hot)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b57fe4f3-3b19-43ac-9927-219c69b912cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 4s 77ms/step - loss: 0.9450 - accuracy: 0.7715\n",
      "Loss no conjunto de validação: 0.9450171589851379\n",
      "Accuracy no conjunto de validação: 0.7714776396751404\n"
     ]
    }
   ],
   "source": [
    "# Avaliar no conjunto de validação\n",
    "val_loss, val_accuracy = model.evaluate(np.array(val_images), val_labels_one_hot)\n",
    "print(f\"Loss no conjunto de validação: {val_loss}\")\n",
    "print(f\"Accuracy no conjunto de validação: {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fbd5aa34-a2f9-4ba0-8f37-f9a63834de27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 4s 73ms/step\n",
      "Matriz de Confusão:\n",
      "[[ 90  13  10  61   5]\n",
      " [ 14 318   7  22   7]\n",
      " [ 17  12 327  73   3]\n",
      " [ 17  27  40 379   8]\n",
      " [  4  36   8  15 233]]\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Classe 0       0.63      0.50      0.56       179\n",
      "    Classe 1       0.78      0.86      0.82       368\n",
      "    Classe 2       0.83      0.76      0.79       432\n",
      "    Classe 3       0.69      0.80      0.74       471\n",
      "    Classe 4       0.91      0.79      0.84       296\n",
      "\n",
      "    accuracy                           0.77      1746\n",
      "   macro avg       0.77      0.74      0.75      1746\n",
      "weighted avg       0.78      0.77      0.77      1746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Obter previsões do modelo\n",
    "val_predictions = model.predict(np.array(val_images))\n",
    "\n",
    "# Converter previsões one-hot encoded para rótulos\n",
    "val_predictions_labels = np.argmax(val_predictions, axis=1)\n",
    "val_labels_labels = np.argmax(val_labels_one_hot, axis=1)\n",
    "\n",
    "# Matriz de confusão\n",
    "conf_matrix = confusion_matrix(val_labels_labels, val_predictions_labels)\n",
    "print(\"Matriz de Confusão:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Relatório de classificação\n",
    "target_names = ['Classe 0', 'Classe 1', 'Classe 2', 'Classe 3', 'Classe 4']\n",
    "class_report = classification_report(val_labels_labels, val_predictions_labels, target_names=target_names)\n",
    "print(\"Relatório de Classificação:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f605d6-9edd-4e8e-9282-31365f8e2504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
