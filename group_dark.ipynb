{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11eaad7c-360f-4f41-bfd3-b60facdf8bc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dbd7f8c-2b43-4415-8c29-7991aac82577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbf9ce5f-eea4-415d-8a99-90c20af94ab2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available\n"
     ]
    }
   ],
   "source": [
    "# Import OS libraries\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "# Data handling tools\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "\n",
    "\n",
    "# Deep learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense,  BatchNormalization, Activation, Dropout  \n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam , Adamax\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# from googleapiclient.discovery import build\n",
    "# from google.oauth2.credentials import Credentials\n",
    "# from googleapiclient.http import MediaFileUpload\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"GPU is available\" if tf.config.list_physical_devices('GPU') else \"GPU is not available\")\n",
    "\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a260b4e-7612-47db-9c07-8ded449e36ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Diretório onde estão armazenadas as imagens sem ser separadas por pastas\n",
    "image_dir = r'C:\\Users\\bruna\\OneDrive - Universidade do Minho\\Tese Mestrado em Bioinformática\\AGAR_dataset\\AGAR_dataset\\dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f637441-d900-4743-8151-642cf70ec9b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_image(image_path, target_size=(112, 112)):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize(target_size)\n",
    "        return np.array(image) / 255.0  # Normaliza a imagem para a faixa [0, 1]\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar a imagem {image_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eadfc86c-4291-48c0-ab59-a792d8060e74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Caminho para o arquivo de IDs das imagens com fundo dark\n",
    "ids_dark = r'C:\\Users\\bruna\\OneDrive - Universidade do Minho\\Microbialdataset\\dark_images_ids.txt'\n",
    "\n",
    "# Carregar IDs do grupo de treinamento de um arquivo de texto\n",
    "with open(ids_dark, 'r') as file:\n",
    "    ids = [line.strip() for line in file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bbacc69-81ae-4c1e-bbe6-2a4752500b65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de IDs carregados: 9649\n",
      "Primeiros 5 IDs: ['10000', '10001', '10002', '10003', '10004']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Número de IDs carregados: {len(ids)}\")\n",
    "print(f\"Primeiros 5 IDs: {ids[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d306e92-75e4-40b7-91ee-4f88aef5375e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dividir os IDs em treinamento e validação (por exemplo, 80% para treinamento e 20% para validação)\n",
    "split_index = int(len(ids) * 0.8)\n",
    "train_ids = ids[:split_index]\n",
    "val_ids = ids[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07f0619c-0778-44c2-a4b0-8d57f37b8d8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Listas para armazenar caminhos de imagem e rótulos correspondentes\n",
    "train_data = []\n",
    "val_data = []\n",
    "\n",
    "# Iterar sobre todos os arquivos no diretório\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.endswith('.json'):\n",
    "        json_path = os.path.join(image_dir, filename)\n",
    "        with open(json_path, 'r') as file:\n",
    "            try:\n",
    "                json_data = json.load(file)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Erro ao carregar JSON {json_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "        sample_id = str(json_data.get('sample_id', None))\n",
    "        if sample_id is None:\n",
    "            print(f\"ID da amostra ausente no JSON: {json_path}\")\n",
    "            continue\n",
    "\n",
    "        image_filename = f\"{sample_id}.jpg\"\n",
    "        image_path = os.path.join(image_dir, image_filename)\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Arquivo de imagem não encontrado: {image_path}\")\n",
    "            continue\n",
    "\n",
    "        classes = json_data.get('classes', [])\n",
    "        if not classes:\n",
    "            print(f\"Chave 'classes' ausente ou vazia no JSON: {json_path}\")\n",
    "            continue\n",
    "\n",
    "        data_tuple = (image_path, classes[0])\n",
    "        if sample_id in train_ids:\n",
    "            train_data.append(data_tuple)\n",
    "        elif sample_id in val_ids:\n",
    "            val_data.append(data_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55c1cf72-3127-4c9f-8ab3-4444865bfea3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de imagens no conjunto de treinamento: 7718\n",
      "Número de imagens no conjunto de validação: 1930\n"
     ]
    }
   ],
   "source": [
    "print(f\"Número de imagens no conjunto de treinamento: {len(train_data)}\")\n",
    "print(f\"Número de imagens no conjunto de validação: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c7a1ac4-c072-40eb-a4e0-5e865301e754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Separar caminhos de imagem e rótulos para treinamento e validação\n",
    "train_images, train_labels = zip(*train_data)\n",
    "val_images, val_labels = zip(*val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95781c8d-06d5-4adc-9a95-b7b9b7350df0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converter os rótulos em formato adequado (numérico), se necessário\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_labels)\n",
    "val_labels = label_encoder.transform(val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7067b356-cb3e-4366-a37a-2bdd2cccd463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converter os rótulos em one-hot encoded\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "train_labels_one_hot = one_hot_encoder.fit_transform(train_labels.reshape(-1, 1))\n",
    "val_labels_one_hot = one_hot_encoder.transform(val_labels.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a57e43cc-accb-4b4d-bdbc-58e238bdf933",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obter o número de classes a partir do one hot encoder\n",
    "num_classes = len(one_hot_encoder.categories_[0])\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a846a64-3d5b-4eb8-94c2-bad4023ed64d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Função para gerar batches de dados\n",
    "def data_generator(image_paths, labels_one_hot, batch_size=8, target_size=(112, 112)):\n",
    "    while True:\n",
    "        for start in range(0, len(image_paths), batch_size):\n",
    "            end = min(start + batch_size, len(image_paths))\n",
    "            batch_paths = image_paths[start:end]\n",
    "            batch_labels = labels_one_hot[start:end]\n",
    "            batch_images = []\n",
    "            for path in batch_paths:\n",
    "                image = load_image(path, target_size)\n",
    "                if image is not None:\n",
    "                    batch_images.append(image)\n",
    "            yield np.array(batch_images), np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f7680bf-bef6-40e2-90e3-edda8d5d9b6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Criar geradores de dados para treinamento e validação\n",
    "train_generator = data_generator(train_images, train_labels_one_hot, batch_size=8, target_size=(112, 112))\n",
    "val_generator = data_generator(val_images, val_labels_one_hot, batch_size=8, target_size=(112, 112))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "111adf45-0883-4a66-a4bb-b8550662d4f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Criar o modelo da CNN\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(112, 112, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')  # Corrigido para usar `num_classes`\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "886b6e28-1eb0-4bd5-8efe-e8381e32e193",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compilar o modelo\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4194006-81fa-4e4f-820c-69c9bd2fee34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "964/964 [==============================] - 1747s 2s/step - loss: 1.5317 - accuracy: 0.2907 - val_loss: 1.6045 - val_accuracy: 0.1981\n",
      "Epoch 2/10\n",
      "964/964 [==============================] - 2201s 2s/step - loss: 1.4296 - accuracy: 0.3361 - val_loss: 1.6343 - val_accuracy: 0.2822\n",
      "Epoch 3/10\n",
      "964/964 [==============================] - 2750s 3s/step - loss: 1.3718 - accuracy: 0.3695 - val_loss: 1.7043 - val_accuracy: 0.2946\n",
      "Epoch 4/10\n",
      "964/964 [==============================] - 208157s 216s/step - loss: 1.3246 - accuracy: 0.4170 - val_loss: 1.6024 - val_accuracy: 0.2863\n",
      "Epoch 5/10\n",
      "964/964 [==============================] - 2329s 2s/step - loss: 1.2927 - accuracy: 0.4311 - val_loss: 1.5729 - val_accuracy: 0.3081\n",
      "Epoch 6/10\n",
      "964/964 [==============================] - 1915s 2s/step - loss: 1.2580 - accuracy: 0.4445 - val_loss: 1.5849 - val_accuracy: 0.3470\n",
      "Epoch 7/10\n",
      "964/964 [==============================] - 1851s 2s/step - loss: 1.2324 - accuracy: 0.4616 - val_loss: 1.6814 - val_accuracy: 0.3060\n",
      "Epoch 8/10\n",
      "964/964 [==============================] - 1743s 2s/step - loss: 1.2222 - accuracy: 0.4652 - val_loss: 1.9019 - val_accuracy: 0.2666\n",
      "Epoch 9/10\n",
      "964/964 [==============================] - 26721s 28s/step - loss: 1.1714 - accuracy: 0.4939 - val_loss: 1.6464 - val_accuracy: 0.3387\n",
      "Epoch 10/10\n",
      "964/964 [==============================] - 1389s 1s/step - loss: 1.0114 - accuracy: 0.5926 - val_loss: 1.4769 - val_accuracy: 0.4492\n"
     ]
    }
   ],
   "source": [
    "# Treinar o modelo\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_images) // 8,  # Número total de batches\n",
    "    epochs=10,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(val_images) // 8   # Número total de batches de validação\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "835a6418-0653-4079-8d77-ade2d851cd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trained_model.dark\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trained_model.dark\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('trained_model.dark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "251ce8d8-b70a-4061-ab3c-25ce6818446a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Função para gerar batches de dados\n",
    "def data_generator(image_paths, labels_one_hot, batch_size=8, target_size=(112, 112)):\n",
    "    while True:\n",
    "        for start in range(0, len(image_paths), batch_size):\n",
    "            end = min(start + batch_size, len(image_paths))\n",
    "            batch_paths = image_paths[start:end]\n",
    "            batch_labels = labels_one_hot[start:end]\n",
    "            batch_images = []\n",
    "            for path in batch_paths:\n",
    "                image = load_image(path, target_size)\n",
    "                if image is not None:\n",
    "                    batch_images.append(image)\n",
    "            yield np.array(batch_images), np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09094426-c354-4e18-9337-3be28b756fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Criar geradores de dados para validação\n",
    "val_generator = data_generator(val_images, val_labels_one_hot, batch_size=8, target_size=(112, 112))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09d4e027-f75f-4c07-9e5b-21eea8cef0d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Obter todas as imagens e rótulos do gerador de validação\n",
    "val_images_all = []\n",
    "val_labels_all = []\n",
    "for batch_images, batch_labels in val_generator:\n",
    "    val_images_all.extend(batch_images)\n",
    "    val_labels_all.extend(batch_labels)\n",
    "    if len(val_images_all) >= len(val_images):  \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b2eca3b-a22d-4272-b8ba-68952e8f75ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converter para numpy arrays\n",
    "val_images_all = np.array(val_images_all)\n",
    "val_labels_all = np.array(val_labels_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "684e93d4-fae8-43d6-b459-d7f59e808176",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 4s 17ms/step\n",
      "Matriz de Confusão:\n",
      "[[ 21 227  40 178   2]\n",
      " [  7 294   1  15   2]\n",
      " [ 12  31 159 147  11]\n",
      " [  6  43  61 252   3]\n",
      " [  8  76  88 105 141]]\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.04      0.08       468\n",
      "           1       0.44      0.92      0.59       319\n",
      "           2       0.46      0.44      0.45       360\n",
      "           3       0.36      0.69      0.47       365\n",
      "           4       0.89      0.34      0.49       418\n",
      "\n",
      "    accuracy                           0.45      1930\n",
      "   macro avg       0.51      0.49      0.42      1930\n",
      "weighted avg       0.51      0.45      0.40      1930\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Obter previsões do modelo\n",
    "val_predictions = model.predict(val_images_all, batch_size=8)\n",
    "\n",
    "# Converter previsões one-hot encoded para rótulos\n",
    "val_predictions_labels = np.argmax(val_predictions, axis=1)\n",
    "val_labels_labels = np.argmax(val_labels_all, axis=1)\n",
    "\n",
    "# Matriz de confusão\n",
    "conf_matrix = confusion_matrix(val_labels_labels, val_predictions_labels)\n",
    "print(\"Matriz de Confusão:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Relatório de classificação\n",
    "# Converter os valores de one_hot_encoder.categories_[0] para strings\n",
    "target_names = [str(int(label)) for label in one_hot_encoder.categories_[0]]\n",
    "class_report = classification_report(val_labels_labels, val_predictions_labels, target_names=target_names)\n",
    "print(\"Relatório de Classificação:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3afadb1-f240-4b66-bb83-ade8b4dbe360",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conf_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Visualizar a matriz de confusão\u001b[39;00m\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m----> 5\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(conf_matrix, annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlues\u001b[39m\u001b[38;5;124m'\u001b[39m, xticklabels\u001b[38;5;241m=\u001b[39mlabel_encoder\u001b[38;5;241m.\u001b[39mclasses_, yticklabels\u001b[38;5;241m=\u001b[39mlabel_encoder\u001b[38;5;241m.\u001b[39mclasses_)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted Label\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue Label\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'conf_matrix' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Visualizar a matriz de confusão\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853014c5-468d-47cb-a3f2-023670e089be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
