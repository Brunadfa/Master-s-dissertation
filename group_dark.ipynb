{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "11eaad7c-360f-4f41-bfd3-b60facdf8bc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8dbd7f8c-2b43-4415-8c29-7991aac82577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cbf9ce5f-eea4-415d-8a99-90c20af94ab2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available\n"
     ]
    }
   ],
   "source": [
    "# Import OS libraries\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "# Data handling tools\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "\n",
    "\n",
    "# Deep learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense,  BatchNormalization, Activation, Dropout  \n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam , Adamax\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# from googleapiclient.discovery import build\n",
    "# from google.oauth2.credentials import Credentials\n",
    "# from googleapiclient.http import MediaFileUpload\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"GPU is available\" if tf.config.list_physical_devices('GPU') else \"GPU is not available\")\n",
    "\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4a260b4e-7612-47db-9c07-8ded449e36ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Diretório onde estão armazenadas as imagens sem ser separadas por pastas\n",
    "image_dir = r'C:\\Users\\bruna\\OneDrive - Universidade do Minho\\Tese Mestrado em Bioinformática\\AGAR_dataset\\AGAR_dataset\\dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4f637441-d900-4743-8151-642cf70ec9b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_image(image_path, target_size=(112, 112)):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize(target_size)\n",
    "        return np.array(image) / 255.0  # Normaliza a imagem para a faixa [0, 1]\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar a imagem {image_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eadfc86c-4291-48c0-ab59-a792d8060e74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Caminho para o arquivo de IDs das imagens com fundo dark\n",
    "ids_dark = r'C:\\Users\\bruna\\OneDrive - Universidade do Minho\\Microbialdataset\\dark_images_ids.txt'\n",
    "\n",
    "# Carregar IDs do grupo de treinamento de um arquivo de texto\n",
    "with open(ids_dark, 'r') as file:\n",
    "    ids = [line.strip() for line in file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9bbacc69-81ae-4c1e-bbe6-2a4752500b65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de IDs carregados: 9649\n",
      "Primeiros 5 IDs: ['10000', '10001', '10002', '10003', '10004']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Número de IDs carregados: {len(ids)}\")\n",
    "print(f\"Primeiros 5 IDs: {ids[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8d306e92-75e4-40b7-91ee-4f88aef5375e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dividir os IDs em treinamento e validação (por exemplo, 80% para treinamento e 20% para validação)\n",
    "split_index = int(len(ids) * 0.8)\n",
    "train_ids = ids[:split_index]\n",
    "val_ids = ids[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "07f0619c-0778-44c2-a4b0-8d57f37b8d8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Listas para armazenar caminhos de imagem e rótulos correspondentes\n",
    "train_data = []\n",
    "val_data = []\n",
    "\n",
    "# Iterar sobre todos os arquivos no diretório\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.endswith('.json'):\n",
    "        json_path = os.path.join(image_dir, filename)\n",
    "        with open(json_path, 'r') as file:\n",
    "            try:\n",
    "                json_data = json.load(file)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Erro ao carregar JSON {json_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "        sample_id = str(json_data.get('sample_id', None))\n",
    "        if sample_id is None:\n",
    "            print(f\"ID da amostra ausente no JSON: {json_path}\")\n",
    "            continue\n",
    "\n",
    "        image_filename = f\"{sample_id}.jpg\"\n",
    "        image_path = os.path.join(image_dir, image_filename)\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Arquivo de imagem não encontrado: {image_path}\")\n",
    "            continue\n",
    "\n",
    "        classes = json_data.get('classes', [])\n",
    "        if not classes:\n",
    "            print(f\"Chave 'classes' ausente ou vazia no JSON: {json_path}\")\n",
    "            continue\n",
    "\n",
    "        data_tuple = (image_path, classes[0])\n",
    "        if sample_id in train_ids:\n",
    "            train_data.append(data_tuple)\n",
    "        elif sample_id in val_ids:\n",
    "            val_data.append(data_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "55c1cf72-3127-4c9f-8ab3-4444865bfea3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de imagens no conjunto de treinamento: 7718\n",
      "Número de imagens no conjunto de validação: 1930\n"
     ]
    }
   ],
   "source": [
    "print(f\"Número de imagens no conjunto de treinamento: {len(train_data)}\")\n",
    "print(f\"Número de imagens no conjunto de validação: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7c7a1ac4-c072-40eb-a4e0-5e865301e754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Separar caminhos de imagem e rótulos para treinamento e validação\n",
    "train_images, train_labels = zip(*train_data)\n",
    "val_images, val_labels = zip(*val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "95781c8d-06d5-4adc-9a95-b7b9b7350df0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converter os rótulos em formato adequado (numérico), se necessário\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_labels)\n",
    "val_labels = label_encoder.transform(val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7067b356-cb3e-4366-a37a-2bdd2cccd463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converter os rótulos em one-hot encoded\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "train_labels_one_hot = one_hot_encoder.fit_transform(train_labels.reshape(-1, 1))\n",
    "val_labels_one_hot = one_hot_encoder.transform(val_labels.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a57e43cc-accb-4b4d-bdbc-58e238bdf933",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obter o número de classes a partir do one hot encoder\n",
    "num_classes = len(one_hot_encoder.categories_[0])\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1a846a64-3d5b-4eb8-94c2-bad4023ed64d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Função para gerar batches de dados\n",
    "def data_generator(image_paths, labels_one_hot, batch_size=8, target_size=(112, 112)):\n",
    "    while True:\n",
    "        for start in range(0, len(image_paths), batch_size):\n",
    "            end = min(start + batch_size, len(image_paths))\n",
    "            batch_paths = image_paths[start:end]\n",
    "            batch_labels = labels_one_hot[start:end]\n",
    "            batch_images = []\n",
    "            for path in batch_paths:\n",
    "                image = load_image(path, target_size)\n",
    "                if image is not None:\n",
    "                    batch_images.append(image)\n",
    "            yield np.array(batch_images), np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7f7680bf-bef6-40e2-90e3-edda8d5d9b6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Criar geradores de dados para treinamento e validação\n",
    "train_generator = data_generator(train_images, train_labels_one_hot, batch_size=8, target_size=(112, 112))\n",
    "val_generator = data_generator(val_images, val_labels_one_hot, batch_size=8, target_size=(112, 112))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "111adf45-0883-4a66-a4bb-b8550662d4f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Criar o modelo da CNN\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(112, 112, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')  # Corrigido para usar `num_classes`\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "886b6e28-1eb0-4bd5-8efe-e8381e32e193",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compilar o modelo\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4194006-81fa-4e4f-820c-69c9bd2fee34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "964/964 [==============================] - 1695s 2s/step - loss: 1.5412 - accuracy: 0.2829 - val_loss: 1.5969 - val_accuracy: 0.2531\n",
      "Epoch 2/20\n",
      "964/964 [==============================] - 1785s 2s/step - loss: 1.4584 - accuracy: 0.3180 - val_loss: 1.6102 - val_accuracy: 0.2479\n",
      "Epoch 3/20\n",
      "964/964 [==============================] - 1751s 2s/step - loss: 1.3891 - accuracy: 0.3608 - val_loss: 1.4481 - val_accuracy: 0.3517\n",
      "Epoch 4/20\n",
      "964/964 [==============================] - 3262s 3s/step - loss: 1.3167 - accuracy: 0.4145 - val_loss: 1.4070 - val_accuracy: 0.4123\n",
      "Epoch 5/20\n",
      "964/964 [==============================] - 1820s 2s/step - loss: 1.1811 - accuracy: 0.5091 - val_loss: 1.2397 - val_accuracy: 0.5067\n",
      "Epoch 6/20\n",
      "964/964 [==============================] - 2103s 2s/step - loss: 1.0909 - accuracy: 0.5567 - val_loss: 1.2100 - val_accuracy: 0.5405\n",
      "Epoch 7/20\n",
      "964/964 [==============================] - 2053s 2s/step - loss: 1.0012 - accuracy: 0.6012 - val_loss: 1.1098 - val_accuracy: 0.5622\n",
      "Epoch 8/20\n",
      "964/964 [==============================] - 1969s 2s/step - loss: 0.9386 - accuracy: 0.6259 - val_loss: 1.0655 - val_accuracy: 0.5415\n",
      "Epoch 9/20\n",
      "964/964 [==============================] - 1780s 2s/step - loss: 0.8781 - accuracy: 0.6550 - val_loss: 1.1556 - val_accuracy: 0.5415\n",
      "Epoch 10/20\n",
      "964/964 [==============================] - 2746s 3s/step - loss: 0.8138 - accuracy: 0.6777 - val_loss: 1.0441 - val_accuracy: 0.5695\n",
      "Epoch 11/20\n",
      "964/964 [==============================] - 33654s 35s/step - loss: 0.7879 - accuracy: 0.6895 - val_loss: 0.9819 - val_accuracy: 0.6172\n",
      "Epoch 12/20\n",
      "964/964 [==============================] - 1416s 1s/step - loss: 0.7411 - accuracy: 0.7099 - val_loss: 0.9166 - val_accuracy: 0.6421\n",
      "Epoch 13/20\n",
      "964/964 [==============================] - 1413s 1s/step - loss: 0.7213 - accuracy: 0.7162 - val_loss: 0.8978 - val_accuracy: 0.6525\n",
      "Epoch 14/20\n",
      "964/964 [==============================] - ETA: 0s - loss: 0.6668 - accuracy: 0.7488"
     ]
    }
   ],
   "source": [
    "# Treine o modelo\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_images) // 8,  # Número total de batches\n",
    "    epochs=20,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(val_images) // 8   # Número total de batches de validação\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b11e2f-d155-4537-87ee-496fba7773d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835a6418-0653-4079-8d77-ade2d851cd0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453344a0-49f6-47ab-9efa-d4404d315241",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
