{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f8b3ab8-df1d-4602-9a5a-be1834848443",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c23d0e0-7657-4848-a207-27b59b37f44a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_mean = (0.485, 0.456, 0.406) # ImageNet\n",
    "img_std = (0.229, 0.224, 0.225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "418b0344-65a1-479e-aa9a-7f5546081973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#img_mean = (0.406, 0.456, 0.485)\n",
    "#img_std = (0.225, 0.224, 0.229)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fdedc84-614f-4dd4-a94e-d0ad87aa1c1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IN_MOMENTUM = 0.15\n",
    "gpu_id = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e668373-1522-4559-b14a-bc221f92ce0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReflectionConv(nn.Module):\n",
    "    '''\n",
    "        Reflection padding convolution\n",
    "    '''\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super(ReflectionConv, self).__init__()\n",
    "        reflection_padding = int(np.floor(kernel_size / 2))\n",
    "        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.reflection_pad(x)\n",
    "        out = self.conv(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "120a6a86-20ef-4f5d-b354-f9af68ceb71b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    '''\n",
    "        zero-padding convolution\n",
    "    '''\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        conv_padding = int(np.floor(kernel_size / 2))\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=conv_padding)\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "182997ea-46f3-4800-a5b4-f0bec229d176",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.relu = nn.ReLU(inplace=True) # 1\n",
    "\n",
    "        self.identity_block = nn.Sequential(\n",
    "            ConvLayer(in_channels, out_channels // 4, kernel_size=1, stride=1),\n",
    "            nn.InstanceNorm2d(out_channels // 4, momentum=IN_MOMENTUM),\n",
    "            nn.ReLU(),\n",
    "            ConvLayer(out_channels // 4, out_channels // 4, kernel_size, stride=stride),\n",
    "            nn.InstanceNorm2d(out_channels // 4, momentum=IN_MOMENTUM),\n",
    "            nn.ReLU(),\n",
    "            ConvLayer(out_channels // 4, out_channels, kernel_size=1, stride=1),\n",
    "            nn.InstanceNorm2d(out_channels, momentum=IN_MOMENTUM),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.shortcut = nn.Sequential(\n",
    "            ConvLayer(in_channels, out_channels, 1, stride),\n",
    "            nn.InstanceNorm2d(out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.identity_block(x)\n",
    "        if self.in_channels == self.out_channels:\n",
    "            residual = x\n",
    "        else: \n",
    "            residual = self.shortcut(x)\n",
    "        out = out + residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c29ccb8c-52d0-4cd5-8260-5b2d4708adc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Upsample(nn.Module):\n",
    "    '''\n",
    "        Since the number of channels of the feature map changes after upsampling in HRNet.\n",
    "        we have to write a new Upsample class.\n",
    "    '''\n",
    "    def __init__(self, in_channels, out_channels, scale_factor, mode):\n",
    "        super(Upsample, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
    "        self.upsample = nn.Upsample(scale_factor=scale_factor, mode='nearest')\n",
    "        self.instance = nn.InstanceNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.upsample(out)\n",
    "        out = self.instance(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9534b26b-cbe0-4826-8cc9-917e963e8474",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HRNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HRNet, self).__init__()\n",
    "\n",
    "        self.pass1_1 = BasicBlock(3, 16, kernel_size=3, stride=1)\n",
    "        self.pass1_2 = BasicBlock(16, 32, kernel_size=3, stride=1)\n",
    "        self.pass1_3 = BasicBlock(32, 32, kernel_size=3, stride=1)\n",
    "        self.pass1_4 = BasicBlock(64, 64, kernel_size=3, stride=1)\n",
    "        self.pass1_5 = BasicBlock(192, 64, kernel_size=3, stride=1)\n",
    "        self.pass1_6 = BasicBlock(64, 32, kernel_size=3, stride=1)\n",
    "        self.pass1_7 = BasicBlock(32, 16, kernel_size=3, stride=1)\n",
    "        self.pass1_8 = nn.Conv2d(16, 3, kernel_size=3, stride=1, padding=1)\n",
    "        self.pass2_1 = BasicBlock(32, 32, kernel_size=3, stride=1)\n",
    "        self.pass2_2 = BasicBlock(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        self.downsample1_1 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.downsample1_2 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.downsample1_3 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.downsample1_4 = nn.Conv2d(32, 32, kernel_size=3, stride=4, padding=1)\n",
    "        self.downsample1_5 = nn.Conv2d(64, 64, kernel_size=3, stride=4, padding=1)\n",
    "        self.downsample2_1 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.downsample2_2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.upsample1_1 = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample1_2 = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upsample2_1 = nn.Upsample(scale_factor=4, mode='bilinear')\n",
    "        self.upsample2_2 = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "\n",
    "    def forward(self, x):\n",
    "        map1 = self.pass1_1(x)\n",
    "        map2 = self.pass1_2(map1)\n",
    "        map3 = self.downsample1_1(map1)\n",
    "        map4 = torch.cat((self.pass1_3(map2), self.upsample1_1(map3)), 1)\n",
    "        map5 = torch.cat((self.downsample1_2(map2), self.pass2_1(map3)), 1)\n",
    "        map6 = torch.cat((self.downsample1_4(map2), self.downsample2_1(map3)), 1)\n",
    "        map7 = torch.cat((self.pass1_4(map4), self.upsample1_2(map5), self.upsample2_1(map6)), 1)\n",
    "        out = self.pass1_5(map7)\n",
    "        out = self.pass1_6(out)\n",
    "        out = self.pass1_7(out)\n",
    "        out = self.pass1_8(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8c674aa-2725-4223-a178-b341f73620a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_image(image):\n",
    "    ''' \n",
    "        Change image into tensor and normalize it\n",
    "    ''' \n",
    "    #image = Image.open(image)\n",
    "    image = Image.fromarray(image)\n",
    "    transform = transforms.Compose([\n",
    "                        # convert the (H x W x C) PIL image in the range(0, 255) into (C x H x W) tensor in the range(0.0, 1.0) \n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize(img_mean, img_std),   # this is from ImageNet dataset\n",
    "                        ])   \n",
    "\n",
    "    # change image's size to (b, 3, h, w)\n",
    "    image = transform(image)[:3, :, :].unsqueeze(0)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25250913-e0f6-4907-a2ff-28e7eaf1b94e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def im_convert(tensor):\n",
    "    \"\"\" Display a tensor as an image. \"\"\"\n",
    "\n",
    "    image = tensor.to(\"cpu\").clone().detach()\n",
    "    image = image.numpy().squeeze(0)    # change size to (channel, height, width)\n",
    "\n",
    "    '''\n",
    "        tensor (batch, channel, height, width)\n",
    "        numpy.array (height, width, channel)\n",
    "        to transform tensor to numpy, tensor.transpose(1,2,0) \n",
    "    '''\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = image * np.array(img_std) + np.array(img_mean)   # change into unnormalized image\n",
    "    #\n",
    "    #for ch in range(3):\n",
    "    #    low = np.amin(image[:,:,ch])\n",
    "    #    if low < 0.: image[:,:,ch] -= low\n",
    "    #for ch in range(3):\n",
    "    #    high = np.amax(image[:,:,ch])\n",
    "    #    if high > 1.: \n",
    "    #        image[:,:,ch] /= high\n",
    "    #\n",
    "    image = image.clip(0, 1)    # in the previous steps, we change PIL image(0, 255) into tensor(0.0, 1.0), so convert it\n",
    "    return np.array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35ecc958-e16c-499f-982d-e88d15c00806",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_features(image, model, layers=None):\n",
    "    '''\n",
    "        return a dictionary consists of each layer's name and it's feature maps\n",
    "    '''\n",
    "    if layers is None:\n",
    "        layers = {'0': 'conv1_1',   # default style layer\n",
    "                  '5': 'conv2_1',   # default style layer\n",
    "                  '10': 'conv3_1',  # default style layer\n",
    "                  '19': 'conv4_1',  # default style layer\n",
    "                  '21': 'conv4_2',  # default content layer\n",
    "                  '28': 'conv5_1'}  # default style layer\n",
    "    features = {}\n",
    "    x = image\n",
    "    # model._modules is a dictionary holding each module in the model\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)    #  layer(x) is the feature map through the layer when the input is x\n",
    "        if name in layers:\n",
    "            features[layers[name]] = x\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eac56e06-5c4f-45e8-9f09-63941a651bab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_grim_matrix(tensor):\n",
    "    b, c, h, w = tensor.size()\n",
    "    tensor = tensor.view(b * c, h * w)\n",
    "    gram_matrix = torch.mm(tensor, tensor.t())\n",
    "    return gram_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08b24037-e9a8-409b-a6b4-d23e4b50c219",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def style_transfer(content_image, style_image, previous_model):\n",
    "\n",
    "    device = torch.device(gpu_id if torch.cuda.is_available() else \"cpu\")\n",
    "    print('------------------------------------------------------------------')\n",
    "    print('style transfer using ' + str(device) + ' started...')\n",
    "    print('------------------------------------------------------------------')\n",
    "\n",
    "    # get the VGG19's structure except the full-connect layers\n",
    "    VGG = models.vgg19(pretrained=True).features\n",
    "    VGG.to(device)\n",
    "    for parameter in VGG.parameters():\n",
    "        parameter.requires_grad_(False)\n",
    "\n",
    "    style_net = HRNet()\n",
    "    if previous_model is not None:\n",
    "        style_net.load_state_dict(previous_model)\n",
    "    style_net.to(device)\n",
    "\n",
    "    content_image = load_image(content_image)\n",
    "    content_image = content_image.to(device)\n",
    "    style_image = load_image(style_image)\n",
    "    style_image = style_image.to(device)\n",
    "\n",
    "    content_features = get_features(content_image, VGG)\n",
    "    style_features   = get_features(style_image, VGG)\n",
    "\n",
    "    style_gram_matrixs = {layer: get_grim_matrix(style_features[layer]) for layer in style_features}\n",
    "    target = content_image.clone().requires_grad_(True).to(device)\n",
    "\n",
    "    # try to give for con_layers more weight so that can get more detail in output image\n",
    "    style_weights = {'conv1_1': 0.1,\n",
    "                     'conv2_1': 0.2,\n",
    "                     'conv3_1': 0.4,\n",
    "                     'conv4_1': 0.8, # 0.8\n",
    "                     'conv5_1': 1.6} # 1.6\n",
    "\n",
    "    content_weight = 150\n",
    "    style_weight = 0.5  # 0.5-5\n",
    "\n",
    "    optimizer = optim.Adam(style_net.parameters(), lr=5e-3) # 5e-3\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.9)\n",
    "    steps = 500  # 500\n",
    "\n",
    "    content_loss_epoch = []\n",
    "    style_loss_epoch = []\n",
    "    total_loss_epoch = []\n",
    "    output_image = content_image\n",
    "\n",
    "    time_start=time.time()\n",
    "    for epoch in range(0, steps+1):\n",
    "        \n",
    "        scheduler.step()\n",
    "        target = style_net(content_image).to(device)\n",
    "        target.requires_grad_(True)\n",
    "        target_features = get_features(target, VGG)  # extract output image's all feature maps\n",
    "        content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2']) ** 2)    \n",
    "        style_loss = 0\n",
    "        # compute each layer's style loss and add them\n",
    "        for layer in style_weights:\n",
    "           \n",
    "            target_feature = target_features[layer]  # output image's feature map after layer\n",
    "            target_gram_matrix = get_grim_matrix(target_feature)\n",
    "            style_gram_matrix = style_gram_matrixs[layer]\n",
    "\n",
    "            layer_style_loss = style_weights[layer] * torch.mean((target_gram_matrix - style_gram_matrix) ** 2)\n",
    "            b, c, h, w = target_feature.shape\n",
    "            style_loss = style_loss + layer_style_loss / (c * h * w)\n",
    "        \n",
    "        total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "        total_loss_epoch.append(total_loss)\n",
    "        style_loss_epoch.append(style_weight * style_loss)\n",
    "        content_loss_epoch.append(content_weight * content_loss)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        output_image = target\n",
    "        if (epoch+1) % 50 == 0: print(str(epoch+1) + \" epochs elapsed...\")\n",
    "    style_net.eval().cpu()\n",
    "    time_end=time.time()\n",
    "    print('style transfer time cost (s)', int(time_end - time_start))\n",
    "\n",
    "    return im_convert(target), style_net.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f01df-0749-4f60-9054-63ce3ba5f735",
   "metadata": {},
   "source": [
    "__Load images__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1e523c-d0df-49aa-ab8a-85dbf54fbaad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
