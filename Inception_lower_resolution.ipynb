{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3598c1a4-7655-4e69-9ad5-b52bc46e6c9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "43cab2cc-b41b-4a7e-84be-b81ce010449c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Diretório onde estão armazenadas as imagens inteiras\n",
    "image_dir = r'C:\\Users\\bruna\\OneDrive - Universidade do Minho\\Tese Mestrado em Bioinformática\\AGAR_dataset\\AGAR_dataset\\dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "19bbdfbf-9549-481a-96ce-522beb8a0482",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_image(image_path, target_size=(224, 224)):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize(target_size)\n",
    "        return np.array(image) / 255.0  # Normaliza a imagem para a faixa [0, 1]\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar a imagem {image_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6484f660-f6f2-4a3c-a357-cd436b9f10d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Carregar IDs do grupo de treinamento de um arquivo de texto\n",
    "train_ids_file = r'C:\\Users\\bruna\\OneDrive - Universidade do Minho\\Tese Mestrado em Bioinformática\\AGAR_dataset\\AGAR_dataset\\training_lists\\lower_resolution_train.txt'\n",
    "with open(train_ids_file, 'r') as file:\n",
    "    train_ids = [str(id) for id in json.loads(file.read())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a73904d6-a276-4b6d-8228-a998a4a733cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Carregar IDs do grupo de validação de um arquivo de texto\n",
    "val_ids_file = r'C:\\Users\\bruna\\OneDrive - Universidade do Minho\\Tese Mestrado em Bioinformática\\AGAR_dataset\\AGAR_dataset\\training_lists\\lower_resolution_val.txt'\n",
    "with open(val_ids_file, 'r') as file:\n",
    "    val_ids = [str(id) for id in json.loads(file.read())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c4fbbc4-204a-41bb-a7c2-53e5377ee393",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs de treinamento carregados: ['16078', '16831', '16073', '16072', '16830'] ... (3319 no total)\n",
      "IDs de validação carregados: ['14175', '14176', '15540', '14172', '14678'] ... (1107 no total)\n"
     ]
    }
   ],
   "source": [
    "# Verificar se os IDs foram carregados corretamente\n",
    "print(f\"IDs de treinamento carregados: {train_ids[:5]} ... ({len(train_ids)} no total)\")\n",
    "print(f\"IDs de validação carregados: {val_ids[:5]} ... ({len(val_ids)} no total)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bbf2c5bb-24e3-4307-9a9c-3280ae98b3c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definir o número de classes\n",
    "num_classes = 5\n",
    "\n",
    "# Função para mapear IDs para rótulos automaticamente\n",
    "def map_id_to_label(image_id):\n",
    "    # Exemplo de mapeamento: hash do ID mod num_classes\n",
    "    return int(image_id) % num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "595907f8-f8bb-4a2d-8b62-be09fdce03af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Função para carregar e redimensionar a imagem\n",
    "def load_image(image_path, target_size=(224, 224)):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize(target_size)\n",
    "        return np.array(image) / 255.0  # Normaliza a imagem para a faixa [0, 1]\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar a imagem {image_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9e40cc2b-1787-40ea-ace5-8686ff55c641",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Função para carregar as imagens e as labels\n",
    "def load_data(image_id, image_dir, target_size=(224, 224)):\n",
    "    image_path = os.path.join(image_dir, f'{image_id}.jpg')\n",
    "    if not os.path.exists(image_path):\n",
    "        return None, None\n",
    "    \n",
    "    image = load_image(image_path, target_size)\n",
    "    if image is None:\n",
    "        return None, None\n",
    "\n",
    "    label = map_id_to_label(image_id)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e0fd8203-eab8-40e1-82a1-256b8f0d12ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_generator(ids, image_dir, target_size=(224, 224)):\n",
    "    for image_id in ids:\n",
    "        image, label = load_data(image_id, image_dir, target_size)\n",
    "        if image is not None and label is not None:\n",
    "            yield image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5504ccf6-b44f-4be6-a012-1311d166ee7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(image, label):\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    label = tf.cast(label, dtype=tf.int32)\n",
    "    label = tf.one_hot(label, depth=num_classes)\n",
    "    return image, label\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(train_ids, image_dir),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(224, 224, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "    )\n",
    ")\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(val_ids, image_dir),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(224, 224, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "564fe504-50c6-4de6-80e9-f1bedecd1280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aplicar preprocessamento\n",
    "train_dataset = train_dataset.map(preprocess).batch(32).shuffle(buffer_size=len(train_ids))\n",
    "val_dataset = val_dataset.map(preprocess).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9cd0d28e-51f0-4771-b01f-70c5c6fc8e0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagem shape: (224, 224, 3), Label: 3\n",
      "Imagem shape: (224, 224, 3), Label: 1\n",
      "Imagem shape: (224, 224, 3), Label: 3\n",
      "Imagem shape: (224, 224, 3), Label: 2\n",
      "Imagem shape: (224, 224, 3), Label: 0\n"
     ]
    }
   ],
   "source": [
    "# Verificar o output do data generator\n",
    "for image, label in data_generator(train_ids[:5], image_dir):\n",
    "    print(f\"Imagem shape: {image.shape}, Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e1f9fde4-6a7f-47cd-8195-9990386020eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for image, label in train_dataset.take(1):\n",
    "#    print(f'Imagem shape após preprocessamento: {image.numpy().shape}')\n",
    "#    print(f'Label após preprocessamento: {label.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dd1a94fc-7465-48fa-a52e-f97fa7c65dd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Carregar o modelo InceptionV3 pré-treinado, excluindo a última camada\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "base_model = InceptionV3(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6aa0ff07-e929-4f15-a201-628cb24860db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Escolher até onde descongelar as camadas\n",
    "fine_tune_at = 75  # número de camadas a serem descongeladas\n",
    "\n",
    "# Congelar todas as camadas até o fine_tune_at\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9647eb72-24ea-4725-8ed9-b0f6aa1e2726",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adicionar camadas densas no topo do modelo base\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    \n",
    "    # Adicionar camadas densas extras com Dropout\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),  # Dropout de 50% para ajudar a prevenir overfitting\n",
    "    \n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),  # Dropout de 50% para ajudar a prevenir overfitting\n",
    "    \n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d29ea57c-72a0-4dff-a0af-c616fd5230cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ajustar o otimizador Adam com uma taxa de aprendizado menor\n",
    "optimizer = optimizers.Adam(learning_rate=0.0001)  # Reduzir a taxa de aprendizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "01cd7104-d134-4486-b41f-ec2b2881cf9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8aa017e9-ba77-41b3-b221-243adef62801",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definição de callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='model_checkpoint.h5', \n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        verbose=1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d12a6b-5615-480b-871a-879ba5fcf871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar algumas imagens e rótulos do conjunto de treinamento\n",
    "#for image_batch, label_batch in train_dataset.take(1):\n",
    "#    for i in range(5):  # Exibir 5 imagens do lote\n",
    "#        image = image_batch[i]  # Obtém uma imagem do lote\n",
    "#        label = label_batch[i]  # Obtém o respectivo rótulo do lote\n",
    "#        # Reescala a imagem para o intervalo [0, 1] para a visualização\n",
    "#        image_rescaled = (image.numpy() - image.numpy().min()) / (image.numpy().max() - image.numpy().min())\n",
    "#        plt.imshow(image_rescaled)  # Exibe a imagem reescalada\n",
    "#        plt.title(f'Label: {np.argmax(label.numpy())}')  # Corrigido para mostrar a classe correta\n",
    "#        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9e4c6843-5d5f-4cb3-8871-b88b8e24026e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "    104/Unknown - 381s 1s/step - loss: 1.8784 - accuracy: 0.1914\n",
      "Epoch 1: val_loss improved from inf to 1.60997, saving model to model_checkpoint.h5\n",
      "104/104 [==============================] - 508s 2s/step - loss: 1.8784 - accuracy: 0.1914 - val_loss: 1.6100 - val_accuracy: 0.2034\n",
      "Epoch 2/10\n",
      "104/104 [==============================] - ETA: 0s - loss: 1.6107 - accuracy: 0.2074\n",
      "Epoch 2: val_loss did not improve from 1.60997\n",
      "104/104 [==============================] - 504s 2s/step - loss: 1.6107 - accuracy: 0.2074 - val_loss: 1.6108 - val_accuracy: 0.2025\n",
      "Epoch 3/10\n",
      "104/104 [==============================] - ETA: 0s - loss: 1.6086 - accuracy: 0.2101\n",
      "Epoch 3: val_loss did not improve from 1.60997\n",
      "104/104 [==============================] - 476s 2s/step - loss: 1.6086 - accuracy: 0.2101 - val_loss: 1.6137 - val_accuracy: 0.1872\n",
      "Epoch 4/10\n",
      "104/104 [==============================] - ETA: 0s - loss: 1.6115 - accuracy: 0.2058\n",
      "Epoch 4: val_loss improved from 1.60997 to 1.60995, saving model to model_checkpoint.h5\n",
      "104/104 [==============================] - 457s 2s/step - loss: 1.6115 - accuracy: 0.2058 - val_loss: 1.6100 - val_accuracy: 0.1790\n",
      "Epoch 5/10\n",
      "104/104 [==============================] - ETA: 0s - loss: 1.6096 - accuracy: 0.2043\n",
      "Epoch 5: val_loss did not improve from 1.60995\n",
      "104/104 [==============================] - 458s 2s/step - loss: 1.6096 - accuracy: 0.2043 - val_loss: 1.6102 - val_accuracy: 0.1781\n",
      "Epoch 6/10\n",
      "104/104 [==============================] - ETA: 0s - loss: 1.6103 - accuracy: 0.2040\n",
      "Epoch 6: val_loss did not improve from 1.60995\n",
      "104/104 [==============================] - 454s 2s/step - loss: 1.6103 - accuracy: 0.2040 - val_loss: 1.6102 - val_accuracy: 0.1781\n",
      "Epoch 7/10\n",
      "104/104 [==============================] - ETA: 0s - loss: 1.6096 - accuracy: 0.2040\n",
      "Epoch 7: val_loss did not improve from 1.60995\n",
      "104/104 [==============================] - 434s 2s/step - loss: 1.6096 - accuracy: 0.2040 - val_loss: 1.6106 - val_accuracy: 0.1781\n",
      "Epoch 8/10\n",
      "104/104 [==============================] - ETA: 0s - loss: 1.6094 - accuracy: 0.2071\n",
      "Epoch 8: val_loss did not improve from 1.60995\n",
      "104/104 [==============================] - 454s 2s/step - loss: 1.6094 - accuracy: 0.2071 - val_loss: 1.6105 - val_accuracy: 0.1781\n",
      "Epoch 9/10\n",
      "104/104 [==============================] - ETA: 0s - loss: 1.6094 - accuracy: 0.2071\n",
      "Epoch 9: val_loss did not improve from 1.60995\n",
      "104/104 [==============================] - 431s 2s/step - loss: 1.6094 - accuracy: 0.2071 - val_loss: 1.6105 - val_accuracy: 0.1781\n",
      "Epoch 9: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Treinamento do modelo\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69ced9d-46f8-4653-a801-612589de50e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74d32f7-3f91-4d3a-8c02-8fcf6bb1d994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ead8ee6-5290-4d8d-b7c4-57e23748d8f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
